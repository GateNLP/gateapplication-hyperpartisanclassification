'''
Various methods for doing nlp stuff (sentence splitting, tokenization) in a consistent way across scripts.
'''

import sys
import re

punctuation = '.,:;?!"\'\'``+={}[]()#~$--'
stopwords = None


def init_stopwords():
    """
    Initialise the `stopwords` module variable,
    only if not already initialised.
    """

    global stopwords

    if stopwords:
        return

    from nltk.corpus import stopwords as NLTK_STOPWORDS
    # Suppress a "paramiko missing" warning that is
    # generated by _importing_ gensim.
    import warnings
    warnings.filterwarnings("ignore", "paramiko missing")
    from gensim.parsing.preprocessing import STOPWORDS as GENSIM_STOPWORDS

    stopwords = set(NLTK_STOPWORDS.words('english'))
    for w in GENSIM_STOPWORDS:
        stopwords.add(w)
    stopwords.add("'s")
    stopwordlist = list(stopwords)
    for s in stopwordlist:
        stopwords.add(s.capitalize())


re_num_simple = re.compile('^-?[0-9.,]+([eE^][0-9]+)?(th)?$')


def filter_tokens(tokens):
    init_stopwords()
    filtered = [token for token in tokens if token not in punctuation and "_" not in token and token not in stopwords]
    filtered = [re_num_simple.subn("<num>", token)[0] for token in filtered]
    return filtered
